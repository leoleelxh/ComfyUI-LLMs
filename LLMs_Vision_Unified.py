import os
import yaml
import folder_paths
import numpy as np
import torch
from PIL import Image
import io
import base64

class LLMs_Vision_Unified:
    """ç»Ÿä¸€çš„è§†è§‰æ¨¡å‹èŠ‚ç‚¹"""
    
    def __init__(self):
        self.config = self._load_config()
        self.model_lists = {}
        # é¢„å…ˆåŠ è½½æ‰€æœ‰æ¨¡å‹ç±»å‹çš„å­æ¨¡å‹åˆ—è¡¨
        for model_type in self.config['chatllmleoleexh']['vision_models']:
            self.model_lists[model_type] = self.config['chatllmleoleexh']['vision_models'][model_type]['model_list']
    
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰èŠ‚ç‚¹è¾“å…¥ç±»å‹"""
        config = cls._load_config_static()
        vision_models = config['chatllmleoleexh']['vision_models']
        
        # è·å–æ‰€æœ‰æ¨¡å‹ç±»å‹
        model_types = list(vision_models.keys())
        
        # è·å–æ‰€æœ‰å¯ç”¨çš„å­æ¨¡å‹
        all_models = []
        for model_type in model_types:
            all_models.extend(vision_models[model_type]['model_list'])
        
        return {
            "required": {
                "image": ("IMAGE",),
                "model_type": (model_types,),
                "model": (all_models,),  # æ‰€æœ‰å¯ç”¨çš„å­æ¨¡å‹
                "prompt": ("STRING", {
                    "multiline": True, 
                    "default": "Please provide a detailed description of this image, including:\n- The main subject(s) and their appearance\n- The setting and environment\n- Colors, lighting, and visual elements\n- Any notable details or unique features\n- The overall mood and atmosphere\n\nDescribe as if you are explaining the image to someone who cannot see it."
                }),
            }
        }
    
    @classmethod
    def _load_config_static(cls):
        """é™æ€æ–¹æ³•åŠ è½½é…ç½®"""
        config_path = os.path.join(folder_paths.base_path, "custom_nodes", "ComfyUI-LLMs", "settings.yaml")
        if not os.path.exists(config_path):
            return {
                "chatllmleoleexh": {
                    "vision_models": {
                        "openai": {"model_list": ["gpt-4-vision-preview"]},
                        "glm4": {"model_list": ["glm-4v"]},
                        "ali": {"model_list": ["qwen-vl-plus"]},
                        "gemini": {"model_list": ["gemini-pro-vision"]}
                    }
                }
            }
            
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    RETURN_TYPES = ("STRING",)
    FUNCTION = "process_image"
    CATEGORY = "LLMs"

    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        return self._load_config_static()
    
    def _get_vision_config(self, model_type):
        """è·å–æŒ‡å®šè§†è§‰æ¨¡å‹çš„é…ç½®"""
        try:
            return self.config['chatllmleoleexh']['vision_models'][model_type]
        except KeyError:
            raise ValueError(f"æœªæ‰¾åˆ°æ¨¡å‹ç±»å‹ {model_type} çš„é…ç½®")
    
    def _encode_image(self, image_tensor):
        """å°†å›¾åƒç¼–ç ä¸ºbase64å­—ç¬¦ä¸²"""
        if isinstance(image_tensor, torch.Tensor):
            image_tensor = image_tensor.cpu()
            image = Image.fromarray(np.clip(image_tensor.squeeze().numpy() * 255, 0, 255).astype(np.uint8))
        elif isinstance(image_tensor, np.ndarray):
            image = Image.fromarray(np.clip(image_tensor * 255, 0, 255).astype(np.uint8))
        else:
            raise ValueError("ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼")
        
        buffered = io.BytesIO()
        image.save(buffered, format="PNG")
        return base64.b64encode(buffered.getvalue()).decode()
    
    def process_image(self, image, model_type, model, prompt):
        """å¤„ç†å›¾åƒå¹¶è¿”å›æè¿°"""
        model_config = self._get_vision_config(model_type)
        encoded_image = self._encode_image(image)
        
        # éªŒè¯æ‰€é€‰æ¨¡å‹æ˜¯å¦å±äºå½“å‰æ¨¡å‹ç±»å‹
        if model not in self.model_lists[model_type]:
            available_models = ", ".join(self.model_lists[model_type])
            return (f"é”™è¯¯ï¼šæ‰€é€‰æ¨¡å‹ {model} ä¸å±äº {model_type} ç±»å‹ã€‚å¯ç”¨çš„æ¨¡å‹æœ‰ï¼š{available_models}",)
        
        # æ›´æ–°é…ç½®ä¸­çš„æ¨¡å‹
        model_config = dict(model_config)
        model_config['model_list'] = [model]
        
        # æ ¹æ®ä¸åŒæ¨¡å‹ç±»å‹è°ƒç”¨ç›¸åº”çš„API
        try:
            if model_type == "openai":
                from .LLMs_Vision_OpenAI import process_openai
                return (process_openai(encoded_image, prompt, model_config),)
            elif model_type == "glm4":
                from .LLMs_Vison_GLM4 import process_glm4
                return (process_glm4(encoded_image, prompt, model_config),)
            elif model_type == "ali":
                from .LLMs_Vison_Ali import process_ali
                return (process_ali(encoded_image, prompt, model_config),)
            elif model_type == "gemini":
                from .LLMs_Vison_Gemini import process_gemini
                return (process_gemini(encoded_image, prompt, model_config),)
            else:
                return (f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}",)
        except Exception as e:
            return (f"å¤„ç†å›¾åƒæ—¶å‡ºé”™: {str(e)}",)

NODE_CLASS_MAPPINGS = {
    "LLMs_Vision_Unified": LLMs_Vision_Unified
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LLMs_Vision_Unified": "LLMs Vision Unified ğŸ–¼ï¸"
} 